{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437fcd68-831b-48d6-87af-3f905149da90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Generation\n",
    "\n",
    "- CoffeeCo is small and fortunately for us there are only a few main stores. \n",
    "- To begin we’ll prime a temporary SQL view called stores to represent our company’s flagship stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379d8f6d-3c9b-49fd-a62f-f8adaf6a579a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+\n",
      "|name|capacity|opens|closes|\n",
      "+----+--------+-----+------+\n",
      "|   a|      24|    8|    20|\n",
      "|   b|      36|    7|    21|\n",
      "|   c|      18|    5|    23|\n",
      "+----+--------+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "# Delta is a storage layer for data lakes\n",
    "from delta.tables import * \n",
    "# DeltaTable is the main class for Delta tables\n",
    "from delta.tables import DeltaTable \n",
    "\n",
    "# Initialize SparkSession\n",
    "# Create a SparkSession and set the extraClassPath configuration\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "    .appName(\"DataGeneration\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/work/jars/*\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Define the schema for the Store class\n",
    "store_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"capacity\", IntegerType(), True),\n",
    "    StructField(\"opens\", IntegerType(), True),\n",
    "    StructField(\"closes\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create a list of Row objects\n",
    "stores = [\n",
    "    Row(\"a\", 24, 8, 20),\n",
    "    Row(\"b\", 36, 7, 21),\n",
    "    Row(\"c\", 18, 5, 23)\n",
    "]\n",
    "\n",
    "# Create a PySpark DataFrame from the Row objects and schema\n",
    "stores_sdf = spark.createDataFrame(stores, store_schema)\n",
    "stores_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b3a36d-71ce-4dfb-92c8-902f4802ce9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd3ccb1-2cf0-410f-a495-3869eee9d601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b5f6b47-8f9a-4a62-b677-820ede6e7164",
   "metadata": {},
   "source": [
    "### Create Temp View for SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e969e4-6400-4c31-acd1-55734b0f3742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a view for the DataFrame\n",
    "stores_sdf.createOrReplaceTempView(\"stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf37a5-74dc-4713-81bf-1188eaf19f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d09b6018-a537-4b5d-a970-34f6e3e6e0bd",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd1b249-03f5-41ac-87f7-b03a803389e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+\n",
      "|name|capacity|opens|closes|\n",
      "+----+--------+-----+------+\n",
      "|   a|      24|    8|    20|\n",
      "|   b|      36|    7|    21|\n",
      "|   c|      18|    5|    23|\n",
      "+----+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the view\n",
    "spark.sql(\"SELECT * FROM stores\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d3d42-d0dc-445d-8ef0-9828f39c6daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f9e4357-7bb6-4242-bcc5-1d3717b2c22c",
   "metadata": {},
   "source": [
    "## Selection\n",
    "\n",
    "- The process of selection is arguably the most fundamental means of reducing the footprint of the data you are working with. \n",
    "- This concept will be familiar to anyone with working knowledge of SQL.\n",
    "- In a nutshell, selection enables us to reduce the set of rows returned by a query by way of a condition.\n",
    "- Say we wanted to find all the stores open on or after a specific time of day.\n",
    "- Returning Only the Rows that Match the Condition closes >= 22 via Simple Selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74678a84-bb79-47be-83b4-a3a188102b65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+\n",
      "|name|capacity|opens|closes|\n",
      "+----+--------+-----+------+\n",
      "|   c|      18|    5|    23|\n",
      "+----+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the view\n",
    "stores_con = spark.sql(\"SELECT * FROM stores where closes >= 22\")\n",
    "stores_con.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8cf81b-477e-4d5a-8061-77683608e666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab33341a-1a5c-4c17-8783-2d1f764c5ba7",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "- When we select a column in a Dataframe, we have a few options for identifying the column. \n",
    "- There are four distinct ways to provide the target column for the selection.\n",
    "- The symbolic aliases ` and $ are implicit conversations that can be used by importing the implicit functions from the SparkSession:\n",
    "\n",
    "\n",
    "- `df.where(df(\"closes\") >= 22)`\n",
    "- `df.where(col(\"closes\") >= 22)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04659d1a-d2e9-42eb-80dc-16f2c09050cf",
   "metadata": {},
   "source": [
    "The where Clause Is Interchangable with the filter Function of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37391c90-3b4d-4790-8cbb-88634d824ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+\n",
      "|name|capacity|opens|closes|\n",
      "+----+--------+-----+------+\n",
      "|   c|      18|    5|    23|\n",
      "+----+--------+-----+------+\n",
      "\n",
      "+----+--------+-----+------+\n",
      "|name|capacity|opens|closes|\n",
      "+----+--------+-----+------+\n",
      "|   c|      18|    5|    23|\n",
      "+----+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Importing org.apache.spark.sql.functions._ is not necessary in PySpark\n",
    "# Importing spark.implicits._ is not necessary if the code is run in a PySpark shell\n",
    "\n",
    "# Filter the DataFrame using the col function\n",
    "filter = stores_sdf.filter(col(\"closes\") >= 22)\n",
    "\n",
    "# Filter the DataFrame using the DataFrame API\n",
    "where = stores_sdf.where(stores_sdf.closes >= 22)\n",
    "\n",
    "# Show the results\n",
    "filter.show()\n",
    "where.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51347d6c-cca9-4a1e-8834-b0e19c58f718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cd6336a-74e1-4cbd-bd06-a76c2edec2b5",
   "metadata": {},
   "source": [
    "## Projection\n",
    "\n",
    "**Projection as the process of reducing the total number of columns returned by a query.**\n",
    "\n",
    "- Say we want to find all stores where the minimum occupancy is greater than 20. \n",
    "- In this case, we can assume we don’t need to worry about when a store opens or closes,\n",
    "- But rather we want to find the `name` of the store only.\n",
    "\n",
    "\n",
    "- Find all stores with an occupancy greater than 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e76f84f9-cd03-4a02-a37c-a2405d58a3aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|   a|\n",
      "|   b|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the view\n",
    "stores_occ = spark.sql(\"SELECT name FROM stores where capacity > 20\")\n",
    "stores_occ.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b4588-5f68-4a31-925b-fc26d3b2311b",
   "metadata": {},
   "source": [
    "- The query in Listing above  shows you how to use projection and selection together. \n",
    "- The projection dictates which columns will be returned by the query, as seen in the select name, which directs Spark to return only the column labeled name.\n",
    "- The selection portion of the query, which is a fancy filter or conditional predicate, dictates which rows meet the criteria to be returned by the query, as seen in where capacity > 20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba71d34-91d9-4a0d-9cfb-d92b1fef0136",
   "metadata": {},
   "source": [
    "**Let’s see how we can build the same query using the DataFrame API directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1109af0-1468-40b5-a3ac-f58230de0cca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|   a|\n",
      "|   b|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "stores_sdf.select(\"name\") \\\n",
    "            .where(col(\"capacity\") > 20) \\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b790b-b55e-498c-8577-4f51666375c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
