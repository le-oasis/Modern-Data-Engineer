{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437fcd68-831b-48d6-87af-3f905149da90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Joins!\n",
    "\n",
    "- More likely, you will have to stitch data from a few different sources together in order to create the data representation needed to solve the problem at hand.\n",
    "\n",
    "- Joins are common within the data pipeline as a solution to combining data. \n",
    "- These workflows fall under the umbrella of the ETL and can be used whenever you need to strategically combine and transform multiple sources of data into a single consolidated view that can be used to answer more targeted problems.\n",
    "- For example, say we were tasked with creating a job that generates the current available occupancy data for our coffee shops. For the sake of the exercise, let’s say we already have a source of data that emits the number of occupied seats per coffee shop. \n",
    "- We can use this data to join with our coffee stores data to create a new view that we can query to find which store can seat a variable sized party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f2987-3879-4260-b795-254411ffe600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14615489-99a8-4251-bd11-b33ae62f4c45",
   "metadata": {},
   "source": [
    "## Joins\n",
    "You can take advantage of joins to combine two or more sources across a join criteria (eg. special selection predicate). There are a few different ways to join your data and selecting the appropriate join style is critically important to the resulting rows returned post-query.\n",
    "\n",
    "### Join Styles\n",
    "* **Innter Join** - Selects all rows where the conditions are fulfilled across both sides of the join\n",
    "* **Right Join**  - Returns all rows from the right-hand view or dataframe, joining all rows where the conditions are met on the left-hand, or appending nulls when conditions are not met\n",
    "* **Left Join**   - Returns all rows from the left-hand view or dataframe, joining all rows where the conditions are also met on the right-hand side, or appending nulls when conditions are not met\n",
    "* **Semi Join**  - Returns all rows from the left-hand view or dataframe only if the right-hand condition is met (can use to create a selection filter that uses another reference (dataframe or view) without joining any data from the right-hand side of the join. Eg. \"I want to see all stores that have current occupancy data otherwise I am not interested\"\n",
    "* **Full Join**   - Returns all rows across both views or dataframes, filling in null values on either side that doesn't meet the match criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23298c-555f-428e-ac83-559a8bda7829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379d8f6d-3c9b-49fd-a62f-f8adaf6a579a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+\n",
      "|name|capacity|opens|closes|\n",
      "+----+--------+-----+------+\n",
      "|   a|      24|    8|    20|\n",
      "|   b|      36|    7|    21|\n",
      "|   c|      18|    5|    23|\n",
      "+----+--------+-----+------+\n",
      "\n",
      "+---------+---------+\n",
      "|storename|occupants|\n",
      "+---------+---------+\n",
      "|        a|        8|\n",
      "|        b|       20|\n",
      "|        c|       16|\n",
      "|        d|       55|\n",
      "|        e|        8|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "# Delta is a storage layer for data lakes\n",
    "from delta.tables import * \n",
    "# DeltaTable is the main class for Delta tables\n",
    "from delta.tables import DeltaTable \n",
    "\n",
    "# Initialize SparkSession\n",
    "# Create a SparkSession and set the extraClassPath configuration\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "    .appName(\"StoreOccupants\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/work/jars/*\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Define the schema for the Store class.\n",
    "store_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"capacity\", IntegerType(), True),\n",
    "    StructField(\"opens\", IntegerType(), True),\n",
    "    StructField(\"closes\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create a list of Row objects\n",
    "stores = [\n",
    "    Row(\"a\", 24, 8, 20),\n",
    "    Row(\"b\", 36, 7, 21),\n",
    "    Row(\"c\", 18, 5, 23)\n",
    "]\n",
    "\n",
    "# Create a PySpark DataFrame from the Row objects and schema\n",
    "stores_sdf = spark.createDataFrame(stores, store_schema)\n",
    "stores_sdf.show()\n",
    "\n",
    "# Define the schema for the Occupants\n",
    "schema = StructType([\n",
    "  StructField(\"storename\", StringType(), True),\n",
    "  StructField(\"occupants\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create a PySpark DataFrame from the occupants sequence\n",
    "occupants_sdf = spark.createDataFrame([\n",
    "  (\"a\", 8),\n",
    "  (\"b\", 20),\n",
    "  (\"c\", 16),\n",
    "  (\"d\", 55),\n",
    "  (\"e\", 8)\n",
    "], schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "occupants_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b3a36d-71ce-4dfb-92c8-902f4802ce9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02669868-813d-4699-bd54-28e39d8227a1",
   "metadata": {},
   "source": [
    "### Create SQL view "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eff0c62-dda1-4a8a-9bee-adcf179ef155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register the stores and occupants DataFrames as temporary views\n",
    "stores_sdf.createOrReplaceTempView(\"stores\")\n",
    "occupants_sdf.createOrReplaceTempView(\"occupants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d6299-58f0-48d5-8542-cb1e0df301a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f9e4357-7bb6-4242-bcc5-1d3717b2c22c",
   "metadata": {},
   "source": [
    "## Inner Join\n",
    "\n",
    "- The inner join is the simplest to understand and just so happens to also be the default join operation in Spark (given this is usually how people want to join data). \n",
    "- The inner join works by selecting only the rows that meet the join selection criteria across both sides of the data being joined.\n",
    "- Inner joins simply ignore all rows that don’t have a matching join condition. Next up, let’s look at the right join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4ddb4af-2ffa-4a73-a81b-a3df7c4698e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+---------+---------+\n",
      "|name|capacity|opens|closes|storename|occupants|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "|   a|      24|    8|    20|        a|        8|\n",
      "|   b|      36|    7|    21|        b|       20|\n",
      "|   c|      18|    5|    23|        c|       16|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a View.\n",
    "stores_sdf.createOrReplaceTempView(\"stores\")\n",
    "occupants_sdf.createOrReplaceTempView(\"occupants\")\n",
    "\n",
    "# Execute the SparkSQL query \n",
    "query = \"\"\"\n",
    "  SELECT *\n",
    "  FROM stores\n",
    "  INNER JOIN occupants\n",
    "  ON stores.name = occupants.storename\n",
    "\"\"\"\n",
    "\n",
    "# Perfom Join Operation\n",
    "inner_joined = spark.sql(query)\n",
    "inner_joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea0111-add3-4b26-adfd-b44b3ff43488",
   "metadata": {},
   "source": [
    "- The result of our join operation is a new DataFrame that combines all the columns of our two data sources where the join criteria is met. \n",
    "- In this case, that’s where there is a matching store name across both data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b790b-b55e-498c-8577-4f51666375c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cced074d-203c-432f-8d55-b3c30003a24c",
   "metadata": {},
   "source": [
    "## Right Join\n",
    "\n",
    "- The right join, or right outer join, returns all rows from the right-side data source explicitly joining all rows where the selection criteria is met with the left side of the data. \n",
    "- When and the data doesn’t match, it will insert null values instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d51785e-2826-4a49-ac08-659d8ba333b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+---------+---------+\n",
      "|name|capacity|opens|closes|storename|occupants|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "|   a|      24|    8|    20|        a|        8|\n",
      "|   b|      36|    7|    21|        b|       20|\n",
      "|   c|      18|    5|    23|        c|       16|\n",
      "|null|    null| null|  null|        d|       55|\n",
      "|null|    null| null|  null|        e|        8|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a View.\n",
    "stores_sdf.createOrReplaceTempView(\"stores\")\n",
    "occupants_sdf.createOrReplaceTempView(\"occupants\")\n",
    "\n",
    "# Write a Query \n",
    "query = \"\"\"\n",
    "  SELECT *\n",
    "  FROM stores\n",
    "  RIGHT JOIN occupants\n",
    "  ON stores.name = occupants.storename\n",
    "\"\"\"\n",
    "\n",
    "# Perfom Join Operation\n",
    "right_joined = spark.sql(query)\n",
    "right_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d8279-c254-491f-87df-a5a66e191f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81cb5be0-10eb-45f6-b6fa-3d65b78546ad",
   "metadata": {},
   "source": [
    "**DataframeAPI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2130d995-c539-4a46-9365-f21781b08879",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+---------+---------+\n",
      "|name|capacity|opens|closes|storename|occupants|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "|   a|      24|    8|    20|        a|        8|\n",
      "|   b|      36|    7|    21|        b|       20|\n",
      "|   c|      18|    5|    23|        c|       16|\n",
      "|null|    null| null|  null|        d|       55|\n",
      "|null|    null| null|  null|        e|        8|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# `stores_sdf` is our stores data DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "right_joined = stores_sdf.join(\n",
    "  occupants_sdf,\n",
    "  col(\"name\") == col(\"storename\"),\n",
    "  \"right\"\n",
    ")\n",
    "\n",
    "right_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a33314c7-9414-4bf0-88d5-bc5521269a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+---------+---------+\n",
      "|name|capacity|opens|closes|storename|occupants|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "|   a|      24|    8|    20|        a|        8|\n",
      "|   b|      36|    7|    21|        b|       20|\n",
      "|   c|      18|    5|    23|        c|       16|\n",
      "|null|    null| null|  null|        d|       55|\n",
      "|null|    null| null|  null|        e|        8|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# `stores_sdf` is our stores data DataFrame\n",
    "right_joined = stores_sdf.join(occupants_sdf, stores_sdf[\"name\"] == occupants_sdf[\"storename\"], \"right\")\n",
    "right_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8bbc9f-440c-432c-82a0-619813b854d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410721f2-0d99-4a57-b35c-698863850233",
   "metadata": {},
   "source": [
    "- In a right join, all rows from the right table (store_occupants in this case) are included in the result, along with matching rows from the left table (stores in this case).\n",
    "- If there are no matching rows in the left table for a given row in the right table, then the result will contain NULL values for the columns of the left table.\n",
    "\n",
    "**Therefore, in this query, the store_occupants table is on the right side of the join, and the stores table is on the left side.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b6006-7784-4f37-b972-85d54ad61b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156900c4-5b3d-494b-ae34-c8f41febca1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253eb7f-afb8-4d80-aa93-a3c1af7e9c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efaf4599-ea39-45e1-8866-2569724d6931",
   "metadata": {},
   "source": [
    "## Left Join \n",
    "\n",
    "- The left join, or left outer join, returns all rows from the left-side data source explicitly joining all rows where the selection criteria is met with the right-side side of the data. When the data doesn’t match, it will insert null values instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4d6b0d9-2961-4630-9a5d-a3cfab86204f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+---------+---------+\n",
      "|name|capacity|opens|closes|storename|occupants|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "|   a|      24|    8|    20|        a|        8|\n",
      "|   b|      36|    7|    21|        b|       20|\n",
      "|   c|      18|    5|    23|        c|       16|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a View.\n",
    "stores_sdf.createOrReplaceTempView(\"stores\")\n",
    "occupants_sdf.createOrReplaceTempView(\"occupants\")\n",
    "\n",
    "# Write a Query \n",
    "query = \"\"\"\n",
    "  SELECT *\n",
    "  FROM stores\n",
    "  LEFT JOIN occupants\n",
    "  ON stores.name = occupants.storename\n",
    "\"\"\"\n",
    "\n",
    "# Perfom Join Operation\n",
    "left_joined = spark.sql(query)\n",
    "left_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4b2b0-3b38-47f0-9165-66cbb0a6f668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5fc821a-19a8-499f-b658-2df605f9efa7",
   "metadata": {},
   "source": [
    "### DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad8edbe3-4d20-48c8-895e-b9257456d99b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+---------+---------+\n",
      "|name|capacity|opens|closes|storename|occupants|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "|   a|      24|    8|    20|        a|        8|\n",
      "|   b|      36|    7|    21|        b|       20|\n",
      "|   c|      18|    5|    23|        c|       16|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left_joined = stores_sdf.join(occupants_sdf, stores_sdf[\"name\"] == occupants_sdf[\"storename\"], \"left\")\n",
    "\n",
    "left_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c774596-7a13-4920-b180-2790c2c8e9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+---------+---------+\n",
      "|name|capacity|opens|closes|storename|occupants|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "|   a|      24|    8|    20|        a|        8|\n",
      "|   b|      36|    7|    21|        b|       20|\n",
      "|   c|      18|    5|    23|        c|       16|\n",
      "+----+--------+-----+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# `stores_sdf` is our stores data DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "leftjoin_ed = stores_sdf.join(occupants_sdf,col(\"name\") == col(\"storename\"),\"left\")\n",
    "\n",
    "leftjoin_ed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb688198-98d9-459f-bd12-ee64548a5e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f8bd28d-6d1d-4055-9458-f184550e47c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|boutiquename|\n",
      "+------------+\n",
      "|           c|\n",
      "+------------+\n",
      "\n",
      "+----+--------+-----+------+\n",
      "|name|capacity|opens|closes|\n",
      "+----+--------+-----+------+\n",
      "|   c|      18|    5|    23|\n",
      "+----+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a view to store coffee shops with fewer than 20 seats\n",
    "boutiques_sdf = stores_sdf.filter(stores_sdf[\"capacity\"] < 20).select(\"name\").withColumnRenamed(\"name\", \"boutiquename\")\n",
    "boutiques_sdf.createOrReplaceTempView(\"boutiques\")\n",
    "\n",
    "# Write a Query \n",
    "query = \"\"\"\n",
    "  SELECT *\n",
    "  FROM boutiques\n",
    "\"\"\"\n",
    "\n",
    "# Execute Query\n",
    "boutique_sql_df = spark.sql(query)\n",
    "boutique_sql_df.show()\n",
    "\n",
    "\n",
    "## Semi Join Operation \n",
    "query = \"\"\"\n",
    "  SELECT * \n",
    "  FROM stores\n",
    "  SEMI JOIN boutiques\n",
    "  ON stores.name = boutiques.boutiquename\n",
    "\"\"\"\n",
    "\n",
    "# Perfom Join Operation\n",
    "semi_joined = spark.sql(query)\n",
    "semi_joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd351b-c9fe-49cd-9a11-b4df0a3e194c",
   "metadata": {},
   "source": [
    "- The result of the operation is that we find the one coffee shop that is a boutique coffee shop. \n",
    "- It just so happens to be the one row from our stores data that intersects with the boutiques data. Let’s look now at the reverse of the semi-join, which is the anti-join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12c212-2084-4c45-98a3-97906a5214d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
