{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3ac87b-6ac9-4485-bfa5-7143f55af654",
   "metadata": {},
   "source": [
    "# First ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3692dc19-0721-4de2-997f-16a591ef5aea",
   "metadata": {},
   "source": [
    "ETL stands for Extract, Transform, and Load. It is a process of integrating data from various sources into a target database or data warehouse by extracting data from the sources, transforming it into a desired format, and loading it into the target system.\n",
    "\n",
    "The ETL process involves the following steps:\n",
    "\n",
    "- Extraction: Data is extracted from various sources, which may include databases, files, web services, etc.\n",
    "- Transformation: The extracted data is transformed into a format that is suitable for analysis and storage. This may involve cleaning and filtering the data, merging it with other data, or performing calculations on it.\n",
    "- Loading: The transformed data is loaded into the target database or data warehouse.\n",
    "\n",
    "The ETL process is a critical component of data integration and is used in various applications such as business intelligence, data warehousing, and data migration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a850a5-75d9-4476-8ef2-1b23471f0929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f010f71-7197-47f8-9543-d5cfb06aa65a",
   "metadata": {},
   "source": [
    "## Pyspark ETL process\n",
    "\n",
    "- In the following example, we will demonstrate a simple ETL process using PySpark. \n",
    "\n",
    "- We will extract data from a CSV file, transform it by adding a new column, and load it into a Parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689bcafd-6460-4db3-ad04-1865d16d28f2",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "663869cc-d13f-42ed-ad18-f2678bd7b7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark import SQLContext\n",
    "import os \n",
    "from delta.tables import * \n",
    "from delta.tables import DeltaTable \n",
    "import hashlib \n",
    "import datetime\n",
    "import urllib.request \n",
    "import json \n",
    "from datetime import timedelta, date\n",
    "from itertools import islice \n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70dea1-0e49-459b-8725-5a783b0fdbff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be22f17a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to create a Spark session\n",
    "\n",
    "def create_spark_session():\n",
    "    return SparkSession.builder.master(\"local[1]\") \\\n",
    "        .appName(\"FirstETL\") \\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/home/jovyan/work/jars/*\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "# Function to extract data from CSV using specified schema\n",
    "\n",
    "def extract_data(spark, input_path):\n",
    "    custom_schema = StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"roast\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", True) \\\n",
    "        .schema(custom_schema) \\\n",
    "        .load(input_path)\n",
    "    \n",
    "    new_df = df.select(\"name\", \"roast\")\n",
    "    return new_df\n",
    "\n",
    "# Function to transform data by adding a timestamp column\n",
    "\n",
    "def transform_data(data_frame):\n",
    "    updated = datetime.today().replace(second=0, microsecond=0)\n",
    "    transformed_df = data_frame.withColumn('updated_at', F.lit(updated))\n",
    "    return transformed_df\n",
    "\n",
    "# Function to load transformed data into Parquet with Snappy compression\n",
    "\n",
    "def load_data(transformed_df, output_folder):\n",
    "    try:\n",
    "        today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        output_path = f\"{output_folder}/{today_date}_curated\"\n",
    "        \n",
    "        transformed_df.write.option(\"compression\", \"snappy\").parquet(output_path)\n",
    "        print(\"Data written successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "\n",
    "# Main function that orchestrates the ETL process\n",
    "\n",
    "def main():\n",
    "    input_path = \"/home/jovyan/work/data/raw-coffee.csv\"\n",
    "    output_folder = \"/home/jovyan/work/data\"\n",
    "    \n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    try:\n",
    "        extracted_data = extract_data(spark, input_path)\n",
    "        transformed_data = transform_data(extracted_data)\n",
    "        load_data(transformed_data, output_folder)\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab1be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6702f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "389892a4",
   "metadata": {},
   "source": [
    "## Learner Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12c812a-8c6f-417a-adcd-7ebbeb73a364",
   "metadata": {},
   "source": [
    "### Initiate SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbfacd4e-6d93-4ada-b8c3-8abac79591f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession from builder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession and set the extraClassPath configuration\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "    .appName(\"FirstETL\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/work/jars/*\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb4b7a0a-4a65-4fee-bd99-c97896a7afdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://myjupyter:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FirstETL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb74c6300a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Details of the Spark Session\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227a121-93b2-48c5-ac57-2e1c6624ae24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d469be39-ede3-4b21-a008-ab14078fc62d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "252439cb-1b61-40e7-b301-fff2cef6143e",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e81e33c-64fe-4cc4-bd8d-1603d24776d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "## TimeStamp Column ##\n",
    "###############################################################\n",
    "UPDATED=datetime.today().replace(second=0, microsecond=0)\n",
    "###############################################################\n",
    "\n",
    "##############################################################\n",
    "## Define Schema ##\n",
    "######################################################################################\n",
    "# Define the schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"roast\", DoubleType(), True)\n",
    "])\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854dce4-bc72-4e2c-9119-d00665c162f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e053f89e-90ba-42e6-afbb-15e57e8bfa29",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ecfb2f9-6761-4f9d-8f30-b7bc8d871065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the data with the specified schema and create a DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(custom_schema) \\\n",
    "    .load(\"/home/jovyan/work/data/raw-coffee.csv\")\n",
    "\n",
    "# Extract relevant columns and create a new DataFrame\n",
    "new_df = df.select(\"name\", \"roast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb9ff3-14ef-47a2-9e96-ce217ebafe3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "768e27fb-5dfd-4950-a0bd-f37aa87277f0",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cf653c3-49bc-4d2f-a085-e41c77c95582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform the data by adding a new column with current timestamp\n",
    "transformed_df = new_df.withColumn('updated_at', f.lit(UPDATED))\n",
    "\n",
    "# Create a new directory with timestamp suffix\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3e422-3bcc-490d-8144-f8f201d46890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e228b76-0984-4c38-bb5c-c8b7cd64ebbb",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b1bece4-a59e-491c-bace-3fdad6fff2da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a new directory with timestamp suffix\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "output_dir = f\"/home/jovyan/work/data/parq_curated_{timestamp}\"\n",
    "transformed_df.write.mode(\"overwrite\").parquet(output_dir)\n",
    "##########################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
