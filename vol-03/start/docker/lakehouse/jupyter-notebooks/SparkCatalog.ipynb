{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867a9b9e-fe43-4d4a-89f3-c9c26a615954",
   "metadata": {},
   "source": [
    "## Spark SQL Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9105bc3e-9114-4cca-813a-c6ce169f4232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta\n",
      "  Downloading delta-0.4.2.tar.gz (4.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: delta\n",
      "  Building wheel for delta (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for delta: filename=delta-0.4.2-py3-none-any.whl size=2928 sha256=330ea2043a65e564af99ab42082bb9448b41fdcdbc81dca746dbcb7a566dcc84\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/06/c9/f4/15ff81c648b9fc73aae5886b41204ada25bd73cbb41b9fad78\n",
      "Successfully built delta\n",
      "Installing collected packages: delta\n",
      "Successfully installed delta-0.4.2\n"
     ]
    }
   ],
   "source": [
    "! pip install delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8207f3e3-526c-4838-be1a-9febe7316b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark\n",
      "  Downloading delta_spark-2.4.0-py3-none-any.whl (20 kB)\n",
      "Collecting pyspark<3.5.0,>=3.4.0\n",
      "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from delta-spark) (4.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.7.0)\n",
      "Collecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285411 sha256=5da642e63d4ea1b2ce7e59a67b561ebfe9704613749287c8d5003c29d944b97e\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/2b/9a/39/d8019ffbfb76a39433455e3d5799e94d3e3cae8f41229f6bf8\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark, delta-spark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.2.0\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "Successfully installed delta-spark-2.4.0 py4j-0.10.9.7 pyspark-3.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba15fe4-10d7-40e9-9100-50d2f155e9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c354293-883c-4311-ae86-fd74feaa99a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf64ce-ff64-4b0c-bb9f-e7f25fadde85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf1c09b-aa43-4fed-b123-fcc29e39d41d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mListDatabasesExample\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.extraClassPath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/work/jars/*\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39menableHiveSupport() \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# List all databases\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m databases \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSHOW DATABASES\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m databases\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ListDatabasesExample\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/work/jars/*\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# List all databases\n",
    "databases = spark.sql(\"SHOW DATABASES\")\n",
    "databases.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78031cdb-a385-40b8-8f24-1ca7d229ef1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='hdfs://namenode:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Spark SQL Catalog to List Databases\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416da1da-3eee-406c-a9bb-932070983e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72c69e-c2a6-4232-bb8e-d51f29824efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf16fb30-0694-4b32-bdd8-79f53931ab65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=====>                                                   (1 + 10) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------+----------------------------------------+\n",
      "|databaseName|description          |locationUri                             |\n",
      "+------------+---------------------+----------------------------------------+\n",
      "|default     |Default Hive database|hdfs://namenode:8020/user/hive/warehouse|\n",
      "+------------+---------------------+----------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert the list of databases to a list of rows\n",
    "rows = [Row(databaseName=db.name, description=db.description, locationUri=db.locationUri) for db in spark.catalog.listDatabases()]\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "df = spark.createDataFrame(rows)\n",
    "\n",
    "# Show the DataFrame without truncating\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1eec3a-673e-470b-886d-e4980c2b5ffe",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8109bac8-a60f-410f-87cb-186859cf6c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+----------+---------+--------------------+\n",
      "| id|             created|             updated|first_name|last_name|               email|\n",
      "+---+--------------------+--------------------+----------+---------+--------------------+\n",
      "|  1|2023-03-19 23:22:...|2023-03-19 23:22:...|      John|      Doe| johndoe@example.com|\n",
      "|  2|2023-03-19 23:22:...|2023-03-19 23:22:...|      Jane|    Smith|janesmith@example...|\n",
      "|  3|2023-03-19 23:22:...|2023-03-19 23:22:...|       Bob|  Johnson|bobjohnson@exampl...|\n",
      "|  4|2023-03-19 23:22:...|2023-03-19 23:22:...|     Alice|      Lee|alicelee@example.com|\n",
      "|  5|2023-03-19 23:22:...|2023-03-19 23:22:...|     David|      Kim|davidkim@example.com|\n",
      "|  6|2023-03-19 23:22:...|2023-03-19 23:22:...|     Linda|   Nguyen|lindanguyen@examp...|\n",
      "|  7|2023-03-19 23:22:...|2023-03-19 23:22:...|      Mike|   Garcia|mikegarcia@exampl...|\n",
      "|  8|2023-03-19 23:22:...|2023-03-19 23:22:...|     Emily|     Chen|emilychen@example...|\n",
      "|  9|2023-03-19 23:22:...|2023-03-19 23:22:...|      Ryan|     Wong|ryanwong@example.com|\n",
      "| 10|2023-03-19 23:22:...|2023-03-19 23:22:...|     Karen|     Zhao|karenzhao@example...|\n",
      "+---+--------------------+--------------------+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the necessary PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configure PostgreSQL JDBC connection properties\n",
    "jdbc_driver = spark.conf.get(\"spark.jdbc.driver.class\", \"org.postgresql.Driver\")\n",
    "db_host = spark.conf.get(\"spark.jdbc.host\", \"oasispostgres\")\n",
    "db_port = spark.conf.get(\"spark.jdbc.port\", \"5432\")\n",
    "default_db = spark.conf.get(\"spark.jdbc.default.db\", \"airflow\")\n",
    "db_table = spark.conf.get(\"spark.jdbc.table\", \"bettercustomers\")\n",
    "db_user = spark.conf.get(\"spark.jdbc.user\", \"airflow\")\n",
    "db_pass = spark.conf.get(\"spark.jdbc.password\", \"airflow\")\n",
    "\n",
    "# Construct the connection URL\n",
    "connection_url = f\"jdbc:postgresql://{db_host}:{db_port}/{default_db}\"\n",
    "\n",
    "# Read the data from the PostgreSQL table\n",
    "better_customers = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", connection_url) \\\n",
    "    .option(\"driver\", jdbc_driver) \\\n",
    "    .option(\"dbtable\", db_table) \\\n",
    "    .option(\"user\", db_user) \\\n",
    "    .option(\"password\", db_pass) \\\n",
    "    .load()\n",
    "\n",
    "# Show the data\n",
    "better_customers.show()\n",
    "\n",
    "# Create or replace a temporary view with the data\n",
    "better_customers.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1433255-c28b-4683-b1be-ba9c2d5c868b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741c994-af59-46c9-a64a-9d4976afaada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f8896a2-c467-4084-8a80-fa6584da85d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_name = \"coffee_co_common\"\n",
    "db_description = \"This database stores common information regarding inventory, stores, and customers\"\n",
    "\n",
    "# Find the default database in the list of databases\n",
    "default_database = None\n",
    "for db in spark.catalog.listDatabases():\n",
    "    if db.name == \"default\":\n",
    "        default_database = db\n",
    "        break\n",
    "\n",
    "if default_database is None:\n",
    "    raise ValueError(\"Default database not found\")\n",
    "\n",
    "default_warehouse = default_database.locationUri\n",
    "warehouse_prefix = f\"{default_warehouse}/common\"\n",
    "\n",
    "# Create the database using Spark SQL\n",
    "spark.sql(f\"\"\"\n",
    "CREATE DATABASE IF NOT EXISTS {db_name}\n",
    "COMMENT '{db_description}'\n",
    "LOCATION '{warehouse_prefix}'\n",
    "WITH DBPROPERTIES(TEAM='core', LEAD='scott', TEAM_SLACK='#help_coffee_common');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c34d87d-7e29-4206-b611-5faf000d5bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2e337-3a6b-4545-9778-c2703e0da451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e3cd092-a273-4e09-8cde-49285744bb02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------------------------------------------------------------------+-----------------------------------------------+\n",
      "|databaseName    |description                                                                       |locationUri                                    |\n",
      "+----------------+----------------------------------------------------------------------------------+-----------------------------------------------+\n",
      "|coffee_co_common|This database stores common information regarding inventory, stores, and customers|hdfs://namenode:8020/user/hive/warehouse/common|\n",
      "|default         |Default Hive database                                                             |hdfs://namenode:8020/user/hive/warehouse       |\n",
      "+----------------+----------------------------------------------------------------------------------+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert the list of databases to a list of rows\n",
    "rows = [Row(databaseName=db.name, description=db.description, locationUri=db.locationUri) for db in spark.catalog.listDatabases()]\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "df = spark.createDataFrame(rows)\n",
    "\n",
    "# Show the DataFrame without truncating\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5addfe-4972-4d2e-a202-4e8739e7e0ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=========================================>               (8 + 3) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|null    |customers|True       |\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tables = spark.catalog.listTables()\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"database\", StringType(), True),\n",
    "    StructField(\"tableName\", StringType(), True),\n",
    "    StructField(\"isTemporary\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Extract relevant data from the Table objects\n",
    "table_data = [(table.database, table.name, str(table.isTemporary)) for table in tables]\n",
    "\n",
    "# Create DataFrame with the table data\n",
    "tables_df = spark.createDataFrame(table_data, schema)\n",
    "tables_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f68c05d-54dc-4185-a723-33a3491d890b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Set the current database to coffee_co_common\n",
    "coffee_co_database_name = \"coffee_co_common\"\n",
    "spark.catalog.setCurrentDatabase(coffee_co_database_name)\n",
    "\n",
    "# Assuming that you have a DataFrame named better_customers\n",
    "# Save the DataFrame as a persistent table in the current database\n",
    "better_customers.write \\\n",
    "    .mode(\"error\") \\\n",
    "    .saveAsTable(\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24535361-944f-4a68-9c7c-0652bbd4190a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-----------+\n",
      "|database        |tableName|isTemporary|\n",
      "+----------------+---------+-----------+\n",
      "|coffee_co_common|customers|False      |\n",
      "|null            |customers|True       |\n",
      "+----------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = spark.catalog.listTables()\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"database\", StringType(), True),\n",
    "    StructField(\"tableName\", StringType(), True),\n",
    "    StructField(\"isTemporary\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Extract relevant data from the Table objects\n",
    "table_data = [(table.database, table.name, str(table.isTemporary)) for table in tables]\n",
    "\n",
    "# Create DataFrame with the table data\n",
    "tables_df = spark.createDataFrame(table_data, schema)\n",
    "tables_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccde1d9-a157-4833-9b5d-cfbf93ce0d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e9e292f-698a-4de0-8a87-4e33ede8de0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the database and table name\n",
    "db_name = \"coffee_co_common\"\n",
    "table_name = \"customers\"\n",
    "\n",
    "# Check if the table exists in the specified database\n",
    "table_exists = spark.catalog._jcatalog.tableExists(db_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8539eb4-4218-463d-9edd-221ce5918764",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fef386-30e5-4955-b88b-7910e00c8497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
