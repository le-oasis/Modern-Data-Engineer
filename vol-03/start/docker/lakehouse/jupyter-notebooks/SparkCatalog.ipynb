{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867a9b9e-fe43-4d4a-89f3-c9c26a615954",
   "metadata": {},
   "source": [
    "## Spark SQL Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9105bc3e-9114-4cca-813a-c6ce169f4232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta\n",
      "  Downloading delta-0.4.2.tar.gz (4.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: delta\n",
      "  Building wheel for delta (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for delta: filename=delta-0.4.2-py3-none-any.whl size=2928 sha256=330ea2043a65e564af99ab42082bb9448b41fdcdbc81dca746dbcb7a566dcc84\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/06/c9/f4/15ff81c648b9fc73aae5886b41204ada25bd73cbb41b9fad78\n",
      "Successfully built delta\n",
      "Installing collected packages: delta\n",
      "Successfully installed delta-0.4.2\n"
     ]
    }
   ],
   "source": [
    "! pip install delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207f3e3-526c-4838-be1a-9febe7316b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark\n",
      "  Downloading delta_spark-2.4.0-py3-none-any.whl (20 kB)\n",
      "Collecting pyspark<3.5.0,>=3.4.0\n",
      "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/310.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:03:37\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba15fe4-10d7-40e9-9100-50d2f155e9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c354293-883c-4311-ae86-fd74feaa99a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'delta.tables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Delta is a storage layer for data lakes\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdelta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# DeltaTable is the main class for Delta tables\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdelta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeltaTable\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'delta.tables'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "# Delta is a storage layer for data lakes\n",
    "from delta.tables import * \n",
    "# DeltaTable is the main class for Delta tables\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf64ce-ff64-4b0c-bb9f-e7f25fadde85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf1c09b-aa43-4fed-b123-fcc29e39d41d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ListDatabasesExample\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/work/jars/*\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# List all databases\n",
    "databases = spark.sql(\"SHOW DATABASES\")\n",
    "databases.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78031cdb-a385-40b8-8f24-1ca7d229ef1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='hdfs://namenode:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Spark SQL Catalog to List Databases\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416da1da-3eee-406c-a9bb-932070983e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72c69e-c2a6-4232-bb8e-d51f29824efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf16fb30-0694-4b32-bdd8-79f53931ab65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=====>                                                   (1 + 10) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------+----------------------------------------+\n",
      "|databaseName|description          |locationUri                             |\n",
      "+------------+---------------------+----------------------------------------+\n",
      "|default     |Default Hive database|hdfs://namenode:8020/user/hive/warehouse|\n",
      "+------------+---------------------+----------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert the list of databases to a list of rows\n",
    "rows = [Row(databaseName=db.name, description=db.description, locationUri=db.locationUri) for db in spark.catalog.listDatabases()]\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "df = spark.createDataFrame(rows)\n",
    "\n",
    "# Show the DataFrame without truncating\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1eec3a-673e-470b-886d-e4980c2b5ffe",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8109bac8-a60f-410f-87cb-186859cf6c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+----------+---------+--------------------+\n",
      "| id|             created|             updated|first_name|last_name|               email|\n",
      "+---+--------------------+--------------------+----------+---------+--------------------+\n",
      "|  1|2023-03-19 23:22:...|2023-03-19 23:22:...|      John|      Doe| johndoe@example.com|\n",
      "|  2|2023-03-19 23:22:...|2023-03-19 23:22:...|      Jane|    Smith|janesmith@example...|\n",
      "|  3|2023-03-19 23:22:...|2023-03-19 23:22:...|       Bob|  Johnson|bobjohnson@exampl...|\n",
      "|  4|2023-03-19 23:22:...|2023-03-19 23:22:...|     Alice|      Lee|alicelee@example.com|\n",
      "|  5|2023-03-19 23:22:...|2023-03-19 23:22:...|     David|      Kim|davidkim@example.com|\n",
      "|  6|2023-03-19 23:22:...|2023-03-19 23:22:...|     Linda|   Nguyen|lindanguyen@examp...|\n",
      "|  7|2023-03-19 23:22:...|2023-03-19 23:22:...|      Mike|   Garcia|mikegarcia@exampl...|\n",
      "|  8|2023-03-19 23:22:...|2023-03-19 23:22:...|     Emily|     Chen|emilychen@example...|\n",
      "|  9|2023-03-19 23:22:...|2023-03-19 23:22:...|      Ryan|     Wong|ryanwong@example.com|\n",
      "| 10|2023-03-19 23:22:...|2023-03-19 23:22:...|     Karen|     Zhao|karenzhao@example...|\n",
      "+---+--------------------+--------------------+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the necessary PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configure PostgreSQL JDBC connection properties\n",
    "jdbc_driver = spark.conf.get(\"spark.jdbc.driver.class\", \"org.postgresql.Driver\")\n",
    "db_host = spark.conf.get(\"spark.jdbc.host\", \"oasispostgres\")\n",
    "db_port = spark.conf.get(\"spark.jdbc.port\", \"5432\")\n",
    "default_db = spark.conf.get(\"spark.jdbc.default.db\", \"airflow\")\n",
    "db_table = spark.conf.get(\"spark.jdbc.table\", \"bettercustomers\")\n",
    "db_user = spark.conf.get(\"spark.jdbc.user\", \"airflow\")\n",
    "db_pass = spark.conf.get(\"spark.jdbc.password\", \"airflow\")\n",
    "\n",
    "# Construct the connection URL\n",
    "connection_url = f\"jdbc:postgresql://{db_host}:{db_port}/{default_db}\"\n",
    "\n",
    "# Read the data from the PostgreSQL table\n",
    "better_customers = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", connection_url) \\\n",
    "    .option(\"driver\", jdbc_driver) \\\n",
    "    .option(\"dbtable\", db_table) \\\n",
    "    .option(\"user\", db_user) \\\n",
    "    .option(\"password\", db_pass) \\\n",
    "    .load()\n",
    "\n",
    "# Show the data\n",
    "better_customers.show()\n",
    "\n",
    "# Create or replace a temporary view with the data\n",
    "better_customers.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1433255-c28b-4683-b1be-ba9c2d5c868b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741c994-af59-46c9-a64a-9d4976afaada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f8896a2-c467-4084-8a80-fa6584da85d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_name = \"coffee_co_common\"\n",
    "db_description = \"This database stores common information regarding inventory, stores, and customers\"\n",
    "\n",
    "# Find the default database in the list of databases\n",
    "default_database = None\n",
    "for db in spark.catalog.listDatabases():\n",
    "    if db.name == \"default\":\n",
    "        default_database = db\n",
    "        break\n",
    "\n",
    "if default_database is None:\n",
    "    raise ValueError(\"Default database not found\")\n",
    "\n",
    "default_warehouse = default_database.locationUri\n",
    "warehouse_prefix = f\"{default_warehouse}/common\"\n",
    "\n",
    "# Create the database using Spark SQL\n",
    "spark.sql(f\"\"\"\n",
    "CREATE DATABASE IF NOT EXISTS {db_name}\n",
    "COMMENT '{db_description}'\n",
    "LOCATION '{warehouse_prefix}'\n",
    "WITH DBPROPERTIES(TEAM='core', LEAD='scott', TEAM_SLACK='#help_coffee_common');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c34d87d-7e29-4206-b611-5faf000d5bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2e337-3a6b-4545-9778-c2703e0da451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e3cd092-a273-4e09-8cde-49285744bb02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------------------------------------------------------------------+-----------------------------------------------+\n",
      "|databaseName    |description                                                                       |locationUri                                    |\n",
      "+----------------+----------------------------------------------------------------------------------+-----------------------------------------------+\n",
      "|coffee_co_common|This database stores common information regarding inventory, stores, and customers|hdfs://namenode:8020/user/hive/warehouse/common|\n",
      "|default         |Default Hive database                                                             |hdfs://namenode:8020/user/hive/warehouse       |\n",
      "+----------------+----------------------------------------------------------------------------------+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert the list of databases to a list of rows\n",
    "rows = [Row(databaseName=db.name, description=db.description, locationUri=db.locationUri) for db in spark.catalog.listDatabases()]\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "df = spark.createDataFrame(rows)\n",
    "\n",
    "# Show the DataFrame without truncating\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5addfe-4972-4d2e-a202-4e8739e7e0ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=========================================>               (8 + 3) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|null    |customers|True       |\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tables = spark.catalog.listTables()\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"database\", StringType(), True),\n",
    "    StructField(\"tableName\", StringType(), True),\n",
    "    StructField(\"isTemporary\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Extract relevant data from the Table objects\n",
    "table_data = [(table.database, table.name, str(table.isTemporary)) for table in tables]\n",
    "\n",
    "# Create DataFrame with the table data\n",
    "tables_df = spark.createDataFrame(table_data, schema)\n",
    "tables_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f68c05d-54dc-4185-a723-33a3491d890b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Set the current database to coffee_co_common\n",
    "coffee_co_database_name = \"coffee_co_common\"\n",
    "spark.catalog.setCurrentDatabase(coffee_co_database_name)\n",
    "\n",
    "# Assuming that you have a DataFrame named better_customers\n",
    "# Save the DataFrame as a persistent table in the current database\n",
    "better_customers.write \\\n",
    "    .mode(\"error\") \\\n",
    "    .saveAsTable(\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24535361-944f-4a68-9c7c-0652bbd4190a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-----------+\n",
      "|database        |tableName|isTemporary|\n",
      "+----------------+---------+-----------+\n",
      "|coffee_co_common|customers|False      |\n",
      "|null            |customers|True       |\n",
      "+----------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = spark.catalog.listTables()\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"database\", StringType(), True),\n",
    "    StructField(\"tableName\", StringType(), True),\n",
    "    StructField(\"isTemporary\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Extract relevant data from the Table objects\n",
    "table_data = [(table.database, table.name, str(table.isTemporary)) for table in tables]\n",
    "\n",
    "# Create DataFrame with the table data\n",
    "tables_df = spark.createDataFrame(table_data, schema)\n",
    "tables_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccde1d9-a157-4833-9b5d-cfbf93ce0d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e9e292f-698a-4de0-8a87-4e33ede8de0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the database and table name\n",
    "db_name = \"coffee_co_common\"\n",
    "table_name = \"customers\"\n",
    "\n",
    "# Check if the table exists in the specified database\n",
    "table_exists = spark.catalog._jcatalog.tableExists(db_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8539eb4-4218-463d-9edd-221ce5918764",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fef386-30e5-4955-b88b-7910e00c8497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
