{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d0bdfa-f6f5-4b53-b0ed-1f25f3226a55",
   "metadata": {},
   "source": [
    "# Data Inception: Using Spark to help Spark\n",
    "\n",
    "- In this following notebook, we will learn to use Spark to help us work more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f1ef0-6930-4708-b13a-be7e5f4b1f0f",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644d4615-a76f-4f99-9252-0fce1b5e22ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PySpark is the main library for Spark\n",
    "import pyspark \n",
    "# SparkContext is the entry point for Spark functionality\n",
    "from pyspark import SparkContext \n",
    "# SparkSession is the entry point for DataFrame and SQL functionality\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91dbc090-3076-4f64-9f48-506756ee4379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Provides a way of using operating system dependent functionality\n",
    "import os \n",
    "# Delta is a storage layer for data lakes\n",
    "from delta.tables import * \n",
    "# DeltaTable is the main class for Delta tables\n",
    "from delta.tables import DeltaTable \n",
    "# Provides cryptographic hashing functions\n",
    "import hashlib \n",
    " # Provides classes for working with dates and times\n",
    "import datetime\n",
    "# Provides functions for working with URLs\n",
    "import urllib.request \n",
    "# Provides functions for working with JSON data\n",
    "import json \n",
    " # Import timedelta and date classes from datetime module\n",
    "from datetime import timedelta, date\n",
    "# Provides functions for working with iterables\n",
    "from itertools import islice \n",
    "# Provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter.\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2713f4f9-a2be-4312-931c-1b74a2aaeda1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Ignore warnings from Apache Spark\n",
    "warnings.filterwarnings(\"ignore\", message=\".*consider reporting.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*illegal-access.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*default log level.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dabdaa-3180-4645-befa-ece626a76322",
   "metadata": {},
   "source": [
    "# Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d52e48-f4cb-4cd3-a5c8-50cb1701a250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2205225e-18a3-4f43-b18e-20b5ebb62a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://myjupyter:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LetSparkWorkForYou</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0559605d80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create SparkSession from builder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession and set the extraClassPath configuration\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "    .appName(\"LetSparkWorkForYou\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/work/jars/*\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Details of the Spark Session\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33464bf6-768c-4619-beb6-34076e69d47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfb105f3-53ee-466b-9b6c-239e4b95a65b",
   "metadata": {},
   "source": [
    "# Read CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ed9ce0-739d-44dc-9f76-df00374a9b81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       name|roast|\n",
      "+-----------+-----+\n",
      "|      yuban|   10|\n",
      "|  nespresso|   10|\n",
      "|     ritual|    4|\n",
      "|four barrel|    5|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffees = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", True) \\\n",
    "                .load(\"/home/jovyan/work/data/raw-coffee.txt\") \\\n",
    "                .toDF(\"name\", \"roast\")\n",
    "\n",
    "coffees.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612d02a-56c5-40e1-b97c-404deebec15f",
   "metadata": {},
   "source": [
    "# Infer Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd6bfe8-7dc2-414b-818c-1369bddf4886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       name|roast|\n",
      "+-----------+-----+\n",
      "|      yuban| 10.0|\n",
      "|  nespresso| 10.0|\n",
      "|     ritual|  4.0|\n",
      "|four barrel|  5.0|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeAndSchema = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .option(\"header\", True) \\\n",
    "    .load(\"/home/jovyan/work/data/raw-coffee.txt\") \\\n",
    "    .toDF(\"name\", \"roast\")\n",
    "\n",
    "coffeeAndSchema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfcda547-a50f-4ab5-bfeb-012d29243da0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- roast: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeAndSchema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5418014-ca6b-4cd5-9059-ee6597c5fc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c500483f-b41f-4961-8833-8c4853a95376",
   "metadata": {},
   "source": [
    "# Manually build The Schema Pattern\n",
    "\n",
    "- When working with critical datasets, using strict schemas enables you to ignore (skip) corrupt data, or to fail fast and kick back an exception, when encountering data that doesnâ€™t conform or parse correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95906112-ea87-4948-8dea-85124d43b75b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       name|roast|\n",
      "+-----------+-----+\n",
      "|      yuban| 10.0|\n",
      "|  nespresso| 10.0|\n",
      "|     ritual|  4.0|\n",
      "|four barrel|  5.0|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"roast\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read the data with the specified schema\n",
    "coffeeAndSchema = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/home/jovyan/work/data/raw-coffee.txt\")\n",
    "\n",
    "# Show the DataFrame\n",
    "coffeeAndSchema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d042242-18cc-4ee9-8f28-e938a02ac745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77193f6f-5377-4872-a358-96f87272a7dc",
   "metadata": {},
   "source": [
    "## SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "714dc562-437a-4b27-9753-1cdf4c6bc76e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       name|roast|\n",
      "+-----------+-----+\n",
      "|      yuban| 10.0|\n",
      "|  nespresso| 10.0|\n",
      "|four barrel|  5.0|\n",
      "|     ritual|  4.0|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a view for the DataFrame\n",
    "coffeeAndSchema.createOrReplaceTempView(\"coffee\")\n",
    "\n",
    "# Query the view\n",
    "spark.sql(\"SELECT * FROM coffee ORDER BY roast desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ffea9-efe3-4cf8-91d0-b35ae61db9cc",
   "metadata": {},
   "source": [
    "## Computing Averages\n",
    "- The task of computing an average is straightforward with Spark SQL (and SQL). You need to simply call the avg expression on a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "481be3cf-c5d8-4755-abd5-ec10dbbbeb26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|avg_roast|\n",
      "+---------+\n",
      "|     7.25|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the view\n",
    "spark.sql(\"SELECT avg(roast) as avg_roast from coffee\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efb4d6-a9cc-47b4-b5b3-d5f16389c007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccc28efc-0c10-4fb5-b9c4-3ec3ba259902",
   "metadata": {},
   "source": [
    "## Test \n",
    "\n",
    "- Find the Min & Max roast values in the table.\n",
    "- Try and sort out the table using the ORDER BY clause.\n",
    "- Try sorting the data by coffee name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb1895-ba59-4f8c-89eb-7227d94d829d",
   "metadata": {},
   "source": [
    "Min & Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d230619-d6eb-40f1-a451-2a3d29852b67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|minimum_roast|maximum_roast|\n",
      "+-------------+-------------+\n",
      "|          4.0|         10.0|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT min(roast) as minimum_roast, max(roast) as maximum_roast from coffee\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a56e4-3073-43d6-9bab-0e995cfc5c7b",
   "metadata": {},
   "source": [
    "Sort the table using the ORDER BY clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff183962-25a4-4e05-99de-1be9f4065ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       name|roast|\n",
      "+-----------+-----+\n",
      "|     ritual|  4.0|\n",
      "|four barrel|  5.0|\n",
      "|      yuban| 10.0|\n",
      "|  nespresso| 10.0|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from coffee order by roast asc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0cbc1-d9dd-4ca0-8a29-1a6c267e5e21",
   "metadata": {},
   "source": [
    "Sort data by coffee name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb27f6de-62f2-4496-8816-c1fa878fbd4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       name|roast|\n",
      "+-----------+-----+\n",
      "|four barrel|  5.0|\n",
      "|  nespresso| 10.0|\n",
      "|     ritual|  4.0|\n",
      "|      yuban| 10.0|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from coffee order by name asc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add6a33-771c-4f55-a3b4-ad07b155da17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecef751-6361-450e-91ce-78c5ca63db99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b9dd2c3-dfa8-440f-8b91-482a56a67df4",
   "metadata": {},
   "source": [
    "# Writing your First Spark ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7041b95-b1e3-4a42-9aec-70268c1d93bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/notebooks'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd9525b3-e324-4fbd-94b1-ef7a9f55d68f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as f\n",
    "##############################################################\n",
    "## TimeStamp Column ##\n",
    "######################################################################################\n",
    "UPDATED=datetime.today().replace(second=0, microsecond=0)\n",
    "######################################################################################\n",
    "\n",
    "##############################################################\n",
    "## Define Schema ##\n",
    "######################################################################################\n",
    "# Define the schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"roast\", DoubleType(), True)\n",
    "])\n",
    "######################################################################################\n",
    "##########################################\n",
    "#### Begin ETL Job #######\n",
    "##########################################\n",
    "# Read the data with the specified schema and create a DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/home/jovyan/work/data/raw-coffee.csv\")\n",
    "##########################################\n",
    "# Extract relevant columns and create a new DataFrame\n",
    "new_df = df.select(\"name\", \"roast\")\n",
    "##########################################\n",
    "# Transform the data by adding a new column with current timestamp\n",
    "transformed_df = new_df.withColumn('updated_at', f.lit(UPDATED))\n",
    "##########################################\n",
    "# Create a new directory with timestamp suffix\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "output_dir = f\"/home/jovyan/work/notebooks/curated_{timestamp}\"\n",
    "transformed_df.write.mode(\"overwrite\").parquet(output_dir)\n",
    "##########################################\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834f924-2b92-4068-a3db-ab512dc05127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b86434-bac0-4d8d-abf2-a532bae708b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb19df0-9844-41c2-ac40-4dd6a524f976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1aab3f-1ad9-4b9a-9dfb-9ac50f2428da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d573387-b79a-4ecf-9076-7ad78d203893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "##############################################################\n",
    "## TimeStamp Column ##\n",
    "######################################################################################\n",
    "UPDATED=datetime.today().replace(second=0, microsecond=0)\n",
    "######################################################################################\n",
    "\n",
    "##############################################################\n",
    "## Define Schema ##\n",
    "######################################################################################\n",
    "# Define the schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"roast\", DoubleType(), True)\n",
    "])\n",
    "######################################################################################\n",
    "##########################################\n",
    "#### Begin ETL Job #######\n",
    "##########################################\n",
    "# Read the data with the specified schema and create a DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/home/jovyan/work/data/raw-coffee.csv\")\n",
    "##########################################\n",
    "# Extract relevant columns and create a new DataFrame\n",
    "new_df = df.select(\"name\", \"roast\")\n",
    "##########################################\n",
    "# Transform the data by adding a new column with current timestamp\n",
    "transformed_df = new_df.withColumn('updated_at', f.lit(UPDATED))\n",
    "##########################################\n",
    "# Create a new directory with timestamp suffix\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "output_dir = f\"/home/jovyan/work/notebooks/curated_{timestamp}\"\n",
    "transformed_df.write.mode(\"overwrite\").parquet(output_dir)\n",
    "\n",
    "\n",
    "df_table_curated.write.option(\"header\",\"true\").csv(\"s3a://silver/CSV/customers\")\n",
    "##########################################\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b82b521-d41d-4ef0-ac98-f1a1495a82c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa7e1219-961e-4989-b726-45aa7ae2868c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "# Define the schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"roast\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Define the updated timestamp\n",
    "UPDATED = datetime.today().replace(second=0)\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"coffeeETL\").getOrCreate()\n",
    "\n",
    "# Read the data with the specified schema and create a DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(custom_schema) \\\n",
    "    .load(\"/home/jovyan/work/data/raw-coffee.csv\")\n",
    "\n",
    "# Extract relevant columns and create a new DataFrame\n",
    "new_df = df.select(\"name\", \"roast\")\n",
    "\n",
    "# Transform the data by adding a new column with current timestamp\n",
    "transformed_df = new_df.withColumn('updated_at', f.lit(UPDATED))\n",
    "\n",
    "# Create a new directory with timestamp suffix\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "output_dir = f\"/home/jovyan/work/notebooks/curate_{timestamp}\"\n",
    "\n",
    "# Write the transformed data to a CSV file\n",
    "transformed_df.write.option(\"header\",\"true\").csv(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7289e0-7278-42f6-8baf-f9d36df7bc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b568b09-c40e-49c0-a867-38787f81ec36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842a07e-614b-4491-9fae-a846d4b5f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.write.mode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
